\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{tocloft}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09


\title{Annotated Bibliography For HTM Researchers}


\author{
Subutai Ahmad \thanks{Send all flames and blame to him.} \\
Numenta, Inc.\\
Redwood City, CA 94063 \\
\texttt{sahmad@numenta.com} \\
\And
Yuwei Cui \\
Department of Biology\\
University of Maryland\\
College Park, MD 20742\\
\texttt{ywcui@umd.edu} \\
}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

% Dots for TOC
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

\begin{document}


\maketitle

\begin{abstract}
This document contains an annotated bibliography targeted towards those doing
active research on Hierarchical Temporal Memory (HTM). The focus is on
neuroscience, though there are occasional other references. The document consists
of (possibly very brief) descriptions of specific papers. These descriptions
emphasize on the relationship to HTM Theory and don't necessarily summarize the
paper itself. There is typically exactly one paper, or a couple of tightly
related papers, for each subsection. We hope the content is useful for finding
appropriate citations when writing HTM related papers. The material could also
be useful in understanding the neuroscience behind the HTM theory in more depth. The
references section includes the abstracts for further details. The bibtex file
for these references are exported from the public Mendeley HTM Neuroscience
group.
 \\

\emph{Date: \today}
\end{abstract}

\clearpage


\renewcommand{\contentsname}{Table of Contents}
\setcounter{tocdepth}{2}
\tableofcontents

\clearpage

\section{Neuroanatomy}

As HTM is a theory of the neocortex, we focused on review papers on architecture of the mammalian cortex, including the laminar and columnar structures of the cortex and the thalamacortical circuits.

\subsection{Thomson: Cortical anatomy}

These papers by Thomson \cite{Thomson2003,Thomson2007,Thomson2010} are dense
but contain a lot of detailed information about the connections into, out of,
and within the various cortical layers and between different types of neurons in the cortex.

\subsection{Buxhoeveden: minicolumn review}

\cite{Buxhoeveden2002} Note from Jeff: \emph{this is the best review article I
know about mini-columns. Start here.}

The article also contains a bunch of data on typical measurements. From this you
can gather that minicolumn widths vary quite significantly between species.  The
width can range from 20-60 microns, with about 40-80 microns distance in between
(center to center).  So there are 12 to 25 minicolumns per mm of cortex.

\subsection{Constantinople and Bruno: parallel systems in the cortex}
This recent paper by Constantinople and Bruno \cite{Constantinople2013} challenged the 
classical belief of sensory processing pathway along $L4\rightarrow L2/3 \rightarrow L5/6$ among cortical layers. 
Instead, it provided new evidence supporting the idea that superficial layers ($L4\rightarrow L2/3$) and 
deeper layers (L5/6) act as parallel systems.

\section{Sparse Distributed Representations}

The HTM theory proposed that sparse distributed representations (SDRs) are used everywhere in the brain. In this section we 
summarized neuroscience research papers supporting this idea.

\subsection{Olshausen: Sparse coding}

The 1996, 1997 papers \cite{Olshausen1996, Olshausen1997} by Olshausen and Field represent the first computational papers on
sparse representations in the cortex. Their work has been very influential in
the machine learning and neuroscience. The 2004 paper \cite{Olshausen2004} is
shorter and easier to read, more of a review.

\subsection{SDR object representations in IT}

\cite{Kiani2007} is an extensive study of SDR-like properties in IT. They
measured responses of 600 neurons in monkey IT to 1000 images. They analyzed the
similarity between all pairs of responses. By doing agglomerative clustering
they show that objects tend to cluster according to natural categories. i.e.
similar objects have similar representations. They use many different distance
methods, though only one is really shown in detail.

The selectivity of single cells was imperfect compared to cell population as a
whole. Many cells discriminated between combinations of categories. In other
words it is a distributed representation.

They showed that lower level simple and complex features (roughly equivalent to
V1, V2) cannot account for these similarity metrics.

Note that each image was only presented for 105ms, so it's essentially a flash
inference case.

\subsection{SDR episodic memory in hippocampus}
This paper \cite{Wixted2014} provides supporting evidence that a sparse distributed neural code 
is used for episodic memory in human hippocampus. Specifically, they showed that  (i) a small 
percentage of recorded neurons responded to any one target and (ii) a small percentage of targets 
elicited a strong response in any one neuron

\subsection{Babadi: Sparseness and Expansion}


The paper \cite{Babadi2014} is tangentially related to our SDR work. I am
including it here only because we might want to cite it in our
theory papers. I think there are several key differences:

\begin{itemize}
\item They are focused on sparseness combined with expansion. This is a bit
strange, since expansion is not required and not found everywhere in the cortex.

\item They require full connectivity matrix between the input layer and the
encoding layer

\item They are using analog weights and a very different distance function
(Euclidean distance instead of overlap)

\end{itemize}

\section{Pyramidal Neurons}

Pyramidal neurons are the most numerous excitatory cells in the cortex and play important roles in advanced cognitive functions. This section contains research and review papers on properties of pyramidal neurons.

\subsection{Spruston: Pyramidal neuron review}

This paper \cite{Spruston2008} contains a very nice review of
pyramidal neurons, including their various regions, active dendritic
properties, number of synapses, etc. They focus on the commonalities across
different cortical areas and hippocampus.

\section{Active Dendrites}

The HTM theory proposed that one important function of the neocortex is to generate predictions of what will happen in the future. Active dendritic properties are likely one of the underlying neural mechanisms of prediction and inference. In this section we reviewed supporting neuroscience evidences of active dendrites.

\subsection{Antic: Decade of NMDA Spikes}
From Jeff: \emph{This is a short and relatively easy to read paper that covers
 the basics of NMDA dendritic spikes including some of the history.  It might be a
good introduction to the topic.}

The paper \cite{Antic2010} describes evidence for the HTM "predictive state". A
non-apical NMDA spike can depolarize a cell body (page 2998, middle of right
column). The time course of this depolarization is interesting - the paper shows
evidence for a more sustained depolarization than we typically model in HTMs.
This could be used to help combat temporal noise or it could support the "learn
on one cell" mode.

They also provide evidence that NMDA spikes are highly localized events
incorporating a small dendritic segment. They are localized in space (10-40
microns) and localized in time. See page 2998, middle of left column.

\subsection{Larkum: Synaptic integration in tuft dendrites}

Although this paper \cite{Larkum2009} is primarily about apical tuft dendrites
it is a good introduction to dendritic spikes in general, including basal
dendrites.  Figure 4H is a good summary diagram of how to think about a typical
pyramidal neuron where the apical tuft dendrites act as a set of coincidence
detectors similar to how the basal dendrites act as a set of coincidence
detectors.  The apical tuft dendrites generate a Ca spike and the basal
dendrites generate a somatic Na action potential.

\subsection{Branco: How dendritic segments integrate their inputs}

This paper \cite{Branco2011} contains some useful data on dendritic segments
and how they integrate their inputs. Summary of findings:

\begin{enumerate}

\item Proximal dendrites sum over their synapses linearly. Synapses sum linearly
and inputs must converge at the exact same time. They also show that more distal
dendritic branches show threshold like (sigmoidal) response properties. This is
nice confirmation for our model.

\item Distal dendrites have broader temporal integration windows. Quote: "We show
that single cortical pyramidal cell dendrites exhibit a gradient of temporal
summation and input gain that increases from proximal to distal locations. This
suggests a progressive shift of computational strategies for synaptic inputs
along single dendrites.

Near the base if inputs are more than a few msecs apart, the evoked potential
drops significantly. Confirmation that proximal dendrites act as coincidence
detectors. Segments farther away from the base can sum over longer temporal
intervals. At the most distal locations, evoked potential stays constant for
much longer than 10 msecs.

Another way to phrase this is that proximal segments require precise synchrony
(coincidence detection), distal segments do not require such precise timing.


\item More distal segments have higher gain (sigmoid gain) and require fewer
synapses to fire. As a consequence, they also state that really distal segments
can be as effective or even more effective than proximal dendrites in driving
axonal output. Not sure what to make of this.


\item They show all this in Layer 2/3 as well as Layer 5 pyramidal cells.

\end{enumerate}

Some reactions to temporal property: We don't model the timing property of
distal segments today, but this property could be very useful. For example
longer temporal scales can be helpful in dealing with temporal noise. A longer
scale will make us more resistant to temporal insertions. It could also be used
to create connections from a single segment to time steps t-1 and t-2. This will
make us more robust to temporal deletions. (This is similar to our old
pooling idea.)


Distal inputs into L4 can be coming in at different time scales (motor commands
could be slower than sensory input). A longer temporal integration period may be
necessary for proper sensorimotor inference.

If apical dendrites have much larger temporal integration windows, this could be
very useful for feedback which necessarily has a slower time scale.

\subsection{Major: Active properties of dendrites}

This paper \cite{Major2013} cites evidence that distal dendritic spikes often
have a weaker modulatory effect. (Proximal connections are thought to have a
stronger effect.)

They review different types of cell models.

They provide numbers regarding the minimum number of synapses required to
trigger dendritic spikes that match up very well with our formal analysis of
SDR‚Äôs.  NMDA spike in distal dendrites can be evoked by as few as 10 active
synapses that are clustered together (page 17, top left).

\subsection{Larkum: BAC firing hypothesis}

Feedback input to apical dendrites by themselves can cause strong action
potentials.   If it is coincident with matching feed forward input, the cell
bursting can be sustained for a period of time. This can happen if apical input
is coincident with matching feed forward input. Mechanisms for that are
reviewed. Figure 3a is an example of this.

\emph{
...this article \cite{Larkum2013} has dealt with the biophysical
evidence for the existence of an associative firing mechanism in pyramidal
neurons and its influence on the input/ output function. This degree of
integration between the micro- and macroarchitecture, as well as inbuilt
complexity at the cellular level, invites speculation about whether and how the
whole system utilizes this feature. The importance of this mechanism
conceptually is that the pyramidal neuron is able to detect coincident input to
proximal and distal dendritic regions, investing the cortex with an inbuilt
associative mechanism at the cellular level for combining feed-forward and
feedback information.}

\emph{The BAC firing hypothesis presented here offers a cellular mechanism that
addresses a number of questions about the cortex. It suggests that the
pyramidal neuron cell type is an associative element which carries out the same
essential task at all cortical stages: that of coupling feed-forward and
feedback information at the cellular level.}


Martinotti neurons blocking dendritic activity - what is this for? Need to
review. Could this turn off pooling when columns burst?



\subsection{Yang: Branch specific dendritic learning}

This amazing paper \cite{Yang2014} studies learning on specific dendritic
branches in awake rats while they go through normal behavior. It shows learning
induced dendritic branch specific formation of synapses, which is consistent
with the temporal memory algorithm and high order sequences. This paper deals
with high order motor sequences. Ignore the sleep part.

\subsection{Palmer: NMDA Spikes}

Few papers talk about dendritic NMDA spikes in Layer 2/3 pyramidal neurons.
That is the topic of this paper \cite{Palmer2014} showing that Layer 2/3 cells
act similarly to the more studied L5 and Hippocampal Ca1 pyramidal neurons.

\section{Synapses and Plasticity}

Learning requires changes of synaptic connections between neurons. In this section we reviewed papers on synaptic plasticity, with a focus on experience-dependent formation and elimination of synapses.

\subsection{Chklovskii: Potential synapses}

This paper \cite{Chklovskii2004} includes evidence and arguments for dynamic
synapse formation,  potential synapses, etc. Jeff says on the NuPIC Theory
mailing list:

\emph{Classic Hebbian learning is about strengthening synapses, but we are
suggesting that new synapses are formed. I think you are asking how this is
possible biologically. The idea for ‚Äúpotential synapses‚Äù comes from a researcher
Chklovskii.  An axon and dendrite that are near each other but that are not
connected can still sense pre- and post-syaptic activity and this is sufficient
for them to start to grow a new synapse when they are active at the same time.
What is actually is grown is a ‚Äúspine‚Äù that connects the axon and dendrite.  The
synapse is on the end of the spine.  In addition, glial cells have been shown to
act as intermediaries.  They can encourage a dendrite and axon to move/grow
closer together when they fire at the same time.  Glial cells effectively
increase the number of potential synapses.  Finally, the ends of the dendritic
and axonal tress are constantly growing I different directions trying to find
new useful connections.  It is now well known that synapses and spines form
during learning and they also disappear.  This can happen rapidly, in a matter
of minutes in some cases.}

\emph{I believe the concept of potential synapses and the growth of new synapses
applies to both proximal and distal synapses.  In our temporal memory
implementation we decided to implement it differently purely to make the
software more efficient and to use less memory.  Instead of maintaining a large
pool of potential synapses, most of which would have 0 permanence, we choose
from the set of active cells.}

\subsection{Experience-dependent synaptogenesis}

The review paper by Bailey and Kandel \cite{Bailey1993} presents numerous early researches supporting synapse formation during formation of long-term memory.

Zito and Svoboda 2002 \cite{Zito2002} is a short review of activity-dependent synaptogenesis in the adult cortex. This review first presented the common view of synaptic plasticity, which argues initial establishment of synaptic connections occurs independent of learning, and experience refines existing synaptic connections, rather than create new ones (para 2, page 1). Zito and Svoboda then presented a different view, which argues "short-term plasticity depends on existing synapses, but long term changes in synaptic strength are accompanied by structural rearrangements, through formation or elimination of synapses. This second view is consistent with the current HTM algorithms.

A more recent review paper by Holtmaat and Svoboda \cite{Holtmaat2009} presents more recent findings with imaging techniques. I find the numbers in supplementary table S1 very useful, it shows what fraction of synapses are stable from many different studies. 

\subsection{Imaging the growth of synapses}

Most recent studies are use fancy image techniques to study synapse formation. Trachtenberg et al \cite{Trachtenberg2002} first shows experience-dependent synapse formation and elimination with long-term imaging. Niell et al. \cite{Niell2004} contains a movie showing how fast synapses and dendritic segments can change dramatically within 24 hours.

\section{Sequence Learning}

In this section we reviewed evidence of sequence learning in the cortex. One prediction of the HTM theory is unexpected input will cause bursting activities in the cortex whereas predicted inputs are represented by SDRs. The other prediction is formation and elimination of lateral connections between neurons are the underlying mechanism of sequence learning. These predictions are supported by the following studies.

\subsection{Vinje and Gallant: Temporal sequences are sparser}

Part of a classic set of studies for HTM sequence learning. This paper
\cite{Vinje2002}
demonstrates increasingly sparse neural activity when a neuron gets larger
spatio-temporal context compared to flash input. Moreover the same set of
neurons tend to fire given the same temporal context. (They show temporal
context by creating‚ that simulate eye saccades.)

There are some problems with the study. For one, the eye movements are not
initiated by the monkey. Instead the movie simulates what the monkey might see
during saccades. This is a huge issue. Their analysis also assumes there are no
temporal correlations in neural activity. Another big problem since temporal
pooling creates temporal correlations. They don't really think in terms of the
monkey learning sequences. Etc.

\subsection{Milier: Visual stimuli recruit intrinsically generated cortical ensembles}

This paper \cite{Miller2014} provides some supporting evidence of the Temporal Memory algorithm. 

Neuroscientists traditionally think that neurons in the cortical encode sensory stimulus individually: each neuron has some preferred stimulus, which can be measured through receptive field (RF) mapping. When a stimulus comes in, a neuron will fire if it matches its RF well, and will stay silent otherwise. 

What's interesting in this paper is that the authors suggest groups (ensembles) of neurons , rather than individual neurons, are the functional units of cortical activity. When a stimulus is presented, cortical activity is dominated by coactive groups of neurons.  Presumably these ensembles are supported via learned recurrent excitatory connections, since they also appears during spontaneous activity.

I find this consistent with the temporal memory algorithm (previously known as the CLA algorithm), which suggest external stimulus triggers a unique sequence of cells, which are connected via lateral connections. However, the technology used in this paper (two-photon imaging) has a relatively poor temporal resolution, so it is not possible to tell whether the "coactive group of neurons" fires sequentially or simultaneously. 

Chetan:
What else is interesting is that they found single neurons to participate in multiple ensembles, regardless of how specialized they were for a stimulus at an individual level. (See section titled "Single Neurons Participate Promiscuously in Multiple Ensembles").

Also, this is interesting: "Taken together, our findings demonstrate that when individual neurons are activated, they are more likely to be activated together with a specific set of other neurons as an ensemble. At the same time, individual neurons can participate in multiple ensembles, dynamically reorganizing their allegiance with different sets of neurons."

\subsection{Meyer: surprise responses in IT}

Subutai: I’ve been trying to locate strong experimental evidence that surprise inputs result in cells in mini column firing (aka bursting).  The attached paper doesn’t quite show that, but it has some other relevant information \cite{Meyer2011}. 
Unpredicted stimuli elicits a stronger response in IT than predicted stimuli. The mean firing rate of neurons is higher with unpredicted stimuli (Figure 2). In figure 2A you do see a slightly increased mean response rate to predicted stimuli (above baseline) but the average response is higher for unpredicted stimuli. All this is consistent with column bursting in temporal memory. In this paper you can't tell whether the surprise is along a minicolumn or not. Additional experiments showed that unpredicted firing had slightly longer latency than predicted firing. This is consistent with predicted cells firing earlier due to depolarization. Predicted firing was about 5 msecs faster than unpredicted firing, which is also about what you would expect.

\subsection{Fishman: Oddball responses in auditory cortex}
 
 From EEG recordings, people know that unexpected stimulus ("deviant" sounds in this study), will elicit stronger response. This paper \cite{Fishman2012} studies the neural mechanisms and brain regions underlying this effect in the primary auditory cortex. They confirmed that spiking response were larger when elicited by the unexpected stimulus, and the difference between expected and unexpected stimulus were more prominent in later activity. The unexpected stimulus can be introduced either in a oddball paradigm in which rare deviant tones are randomly interspersed among frequent standard tones (Fig. 1B), or in a random sequence where tone is unexpected (Fig. 1C). 

One downside of this study is they used a very simple sequence for the oddball paradigm. It will be more interesting to see whether the same phenomenon holds when they play a more complex tone sequence (an oddball within a melody). They conclude that the difference between unexpected stimulus and repeated expected stimulus is due to stimulus specific adaptation (rather than prediction).  

\section{Temporal Pooling and Invariances}

The idea of temporal pooling propose that after learning, predicted inputs will lead to stable (invariant) representations in higher levels of the cortical hierarchy despite changes of inputs at lower levels of the hierarchy. We summarized supporting neuroscience evidences for this idea below.

\subsection{Li and DiCarlo: Learning invariances}

This paper \cite{Li2008} very closely supports HTM temporal pooling ideas,
including the role of sensorimotor inference in forming invariant
representations.  A detailed summary follows:

They found neurons in monkey IT that responded strongly to a particular object “P” (preferred) and moderately to another “N” (non-preferred). Tested position tolerance of these objects to get a baseline.  The difference between the responses to “P” and “N” is the “selectivity” for this neuron.  They did this for about 100 neurons in two monkeys, and many different objects.

They then altered temporal contiguity of stimuli and tested the effects on these invariant representations. 


In the experiment, monkeys viewed altered stimuli for several hours. Basically there were three positions for each object: centered, 3’ above and below. For a given P object, the position above or below was designated the “swap” position. As eyes moved around, a P object would be shown in the swap position but was was replaced by the N object during saccade. (Each saccade lasts 23 msecs.) This would happen every time P object was shown in the swap position.  A P object in the non-swap position didn’t change when the money saccaded. (The swap position was randomly changed for different objects.)  The monkey is effectively blind during a saccade, so they don’t see the change - they just see the new object after fixation.

Their prediction: invariance selectivity at swap position would change. Selectivity in the non-swap position won’t change.  This is what happened. At swap position cells did not discriminate as well between P and N. Neuron became more responsive to N and less responsive to P, but only at that location. Change was location specific and shape specific, so very specific to this particular invariance. These changes cannot be explained by attention affects or retinotopic adaptation (everything was counter balanced). 

However another paper of theirs seemed to indicate that temporal contiguity alone was not enough. Looks like eye movements may also be required (this is their reference 10 - Cox and DiCarlo, 2005. Attached paper 2).

They call this effect “Unsupervised temporal tolerance learning” or UTL. This is just temporal pooling for us.  The behavior they describe is exactly the pooling behavior we would expect in layer 3.

In \cite{Cox2005} they have the same experiment as \cite{Li2008}, but
here they used human subjects and a same-difference psychological test. ¬†
Swapped objects were much more likely to be confused than non-swapped objects.
They did the same experiment without eye saccades, and did not see the effect.¬†

\emph{"Moreover, the confusions are predictable in that they are what is
expected if the visual system assumes that object identity is stable across the short time
interval of a saccade."}


\subsection{Isik et al: Learning invariance using temporal associations}

Yuwei: Here \cite{Isik2012} is a modeling study from Tomaso Poggio's lab that
 is very
relevant to the idea of temporal pooling. This paper aims to explain the
"invariance disruption" experiments, which is the study Subutai found that
supports temporal pooling. In that experiment, Li and DiCarlo
\cite{Li2008} showed that
individual IT neurons change their selectivity in a position-dependent manner
after exposure to the altered visual experiment. The modeling study here used a
temporal association learning rule to learn transformation invariance through
natural visual experience. The basic underlying idea is very similar to temporal
pooling: since the external visual scene usually changes at a slow time scale,
"temporal adjacency" is a good cue that two images are of the same object.

I think the major difference between this study and what we have is they don't
distinguish between predicted and unpredicted inputs. In our algorithm, we only
start temporal pooling when the input is predicted. They also don't have a motor
component in their model. Nevertheless, I feel this line of study (developing
invariance with temporal association rules) is very relevant for us to think
about temporal pooling. i am planning to take a look at some papers along these
lines.

Jeff:  In addition to Yuwei's observations I would add:  - They make a
distinction between generic transformations‚ (such as image translations) and
‚class-specific translations‚ (such as rotation through plane).  They argue that
generic transformations apply to all objects but class-specific transformations
do not.  I am struggling to see if this distinction really exists.  I would
prefer that temporal pooling works the same in all cases and that this
distinction doesn't exist.


Another thing they talk about which I found odd is they ask, why should
training continue throughout life?  They say generic transformations can be set
early in life.  This seems ridiculous to me.  Perhaps they are trying to show
that HMAX (which doesn't learn at all) is a good model.  They show a chart
(figure 2) showing that for translation fixed HMAX is as good as learning
temporal pooling.


\section{Thalamocortical Pathways}

\subsection{Sherman and Guillery: The Book}

For those interested in going deeper into the role of the thalamus, this is an
excellent book \cite{Sherman}. Suggested by Jeff, it is a well written summary
of a modern view of cortico-thalamic connections. It describes, for example, the
connections between every cortical region and the thalamus including the role of
sub-cortical motor centers. It does require some neuroscience background but is
much easier to read than many of the really dense neuroscience papers. The
diagrams are also very clear.

\section{Sensorimotor Inference}

The idea of sensorimotor inference proposed cortex makes prediction of future inputs based on both the current inputs and a copy of motor command that is about to be executed.  Some supporting evidences are summarized below.

\subsection{Bartoli: SDR representations of tools}

This paper \cite{Bartoli2014} is concerned with merging of information from
visual properties of tools to the motor coordinates for using and interacting
with those tools. Looking at premotor and motor populations of neurons coding
for specific hand configurations.

An "affordance" is the set of stuff that can be done to an object.  It is the
possibility of some actions that can be performed on an object.

Neurons in premotor cortex code for object specific and grip specific actions.
(Rochat, 2010). A subset of these neurons fire for executing a specific
hand-object and a specific visual representation of that object. (Murata 97)
This activity is indepedenent of whether that action actually occurs later.

This paper shows that the same specific patterns observed in monkeys also occurs
in humans.  They visually show specific tools to people, and observe very
specific "motor plans" arising as a result of the stimuli. The main contribution
is to "close the gap" between monkey and human data. In addition, they show that
very specific visuomotor representations can form in humans as soon as 150 msecs
after visual presentation of the object.  The did a TMS study, so the stimulated
premotor cortex (they did not perform direct neural recordings).  They did
record activity from hand muscles.

\subsection{Wolpert: Forward models}

This paper \cite{Wolpert1996} discussed sensorimotor inference from a computational perspective. They proposed a forward model that uses the current state of the motor system and motor command to predict the next state. This is similar to our sensorimotor inference algorithms. The forward model concept in this paper is widely used in motor control and sensorimotor inference.

\subsection{Sommer: Efference Copy}

This review paper \cite{Sommer2006} summarizes a series of studies that
established a  pathway for corollary discharge signal (the motor command copy to
sensory cortex), explains how predictive shifting of receptive field is
constructed with CD signal, and how visual stability is achieved despite
eye-movements.

\section{Related Computational Models}

In this section we reviewed several computational models that is related to the HTM theory.

\subsection{Rinkus - The Sparsey model}

I have read through the paper \cite{Rinkus2014} and summarized some relevant
points below. I tried to do this by focusing on the ideas instead of the
algorithm details (which I am still struggling with). It is indeed a hard paper
to read. The author is writing in his own language and often refer to terms not
defined yet in the text. It is more like a technical report than a well-written
journal article (a very bad introduction at the beginning). Nevertheless, I
think some of the ideas presented in this paper are nice and largely consistent
with what we have now.

\subsubsubsection{Overall Goal}

I think it might be good to state the goal of this
work at the beginning, which is somewhat similar to ours. First, it is desired
to have the ability to form large numbers of permanent memory traces of
arbitrary spatiotemporal events on-the-fly and based on single trials. Second,
the ability to subsequently directly, without serial search, retrieve the
best-matching memory given an input, this retrieval should be invariance to
nonlinear time-warping (i.e., if you speed up or slow down the input
nonlinearly). Finally, the author also want the algorithm to be somewhat
biologically plausible, relying on Hebbian-like learning rules rather than
gradient calculation like deep-learning approaches.

\subsubsubsection{Sparse Distributed Code (SDC)}.

This is the key component of the
entire paper.
The author has a good understanding on this topic and proposed something very
similar to our algorithm: SDC must be used everywhere along the cortical
hierarchy. The author provides many neuroscience evidences supporting this idea
(e.g., right bottom of Page 6 on SDC in IT), and compares localist codes used in
the HMAX framework with the SDC (Fig I-4, the section Sparse distributed codes
and Localist Codes at bottom of Page 7, and discussion on Page 40-41). I think
this is a good paper to cite for Subutai's SDR paper. Some similar points are
made with SDC, such as why this is important for from a capacity point of view.

\subsubsubsection{Macrocolumn, ("MAC")}.

The author acknowledged the existence of
mini-columns
(which he refers as "competitive-modules", or CM), but he focused on
macro-columns as the basic coding unit in his algorithm. I find the definition
of MAC fuzzy and somewhat arbitrary. It is basically a collection of \~{}70
minicolumns and correspond to "hypercolumns" in V1. I think we can get something
similar by having topology turned on in our algorithms. Nevertheless, this
design forced one to think in terms of distributed representation, rather than
individual cells. The author had an interesting discussion on a novel concept of
"receptive field of a MAC as a whole" at the bottom of Page 3.

\subsubsubsection{Contextual input from horizontal and feedback connections.}

This
is another
advance over the HMAX framework and is consistent with our algorithm. There are
three sets of inputs to each MAC: bottom-up (U), horizontal (H), and top-down
(D) inputs. Each MAC combines the three inputs to yield a scalar judgement G,
which represents the familiarity of the current input (bottom of page 2) and can
be used for retrieval/recognition purpose. At first glance, I don't think this
is a biologically plausible operation as it depends on the global state of a
MAC, but maybe some local neural network mechanism can do that. Nevertheless,
the inclusion of contextual inputs makes the algorithms suitable for "sequence
memory"

\subsubsubsection{Code Selection Algorithm.}

During the learning, the key problem is
 how to
choose a SDC code for each MAC given the current input. The goal is to achieve a
similar inputs map to similar codes (SISC) principle (Fig. I-6), just like what
we what to achieve with the spatial pooling algorithm. The novel part of his
algorithm is contextual inputs are also considered during spatial pooling (now I
am using our terminology). The idea is to extend the similarity metric to
multiplicatively combine overlap similarity metrics of the U, H and D inputs.

\subsubsubsection{Invariance to nonlinear time warping}

This is temporal noise in our
terms. As
stated in 0, one of the goals in this paper is to address the nonlinear time
warping problem (e.g., consider inputs [BOUNDARY], [BOUNDRY] and
[BOUNNNNNNDARY], should they be regarded the same?). The problem is addressed by
a trick. At each step, a mac computes a series of estimates of the match of the
current temporal-context-dependent input not just to the set of actual moments
it experienced during learning (which constitute its explicit spatiotemporal
basis), but to a much larger space of variants of the basis moments that were
not actually experienced (see Discussion on Page 22). Personally I don't think
this is a great solution, but could be food for thoughts when we are working on
the temporal noise problem.


\bibliographystyle{plain-annote}
%\bibliographystyle{apalike}
\bibliography{htm_bibliography}

\end{document}

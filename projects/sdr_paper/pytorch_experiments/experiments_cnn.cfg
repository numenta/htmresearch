[DEFAULT]

repetitions = 1
iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.01
learning_rate_factor = 1.0
use_batch_norm = False
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
seed = 42
n = 100
k = 100
weight_sparsity = 1.0
weight_sparsity_cnn = 1.0
k_inference_factor = 1.0

no_cuda = False             # If True, disables CUDA training
log_interval = 1000         # how many minibatches to wait before logging
create_plots = False
test_noise_every_epoch = True # If False, will only test noise at end

; validation dataset ratio. Train on X%, validate on (1-X)%.
; Set to 1.0 to skip the validation step and use the whole training dataset
validation = float(50000.0/60000.0)

path = results
datadir = "data"
saveNet = False

optimizer = SGD

; Learning Rate Scheduler. See "torch.optim.lr_scheduler" for valid class names
lr_scheduler = "StepLR"

; Configure lr_scheduler class constructor using kwargs style dictionary
lr_scheduler_params = "{'step_size': 1, 'gamma':%(learning_rate_factor)s}"

; CNN specific parameters
use_cnn = True
c1_out_channels = 20
c1_k = 2000000
dropout = 0.0

c1_input_shape = "1_28_28"

;[cnnQuick]
;c1_out_channels = 3
;c1_k = 10
;n = 30
;k = 10
;iterations = 5
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.02
;learning_rate_factor = 0.85
;weight_sparsity = 0.4
;weight_sparsity_cnn = 0.4
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = False
;batches_in_epoch = 40
;create_plots = False
;batch_size = 4
;validation = 0.95

; This one should get about 98.5% accuracy and about 100K in totalCorrect
;[bestSparseCNNOneLayer]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 50
;iterations = 9
;boost_strength = 1.4
;boost_strength_factor = 0.85
;learning_rate = 0.02
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0

;[bestSparseCNNOneLayerSeeds]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 50
;iterations = 9
;boost_strength = 1.4
;boost_strength_factor = 0.85
;learning_rate = 0.02
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Dense CNN layers with a sparse hidden layer identical to Sparse CNN2
;[denseCNN2SP3Seeds]
;c1_out_channels = "30_30"
;c1_k = "4320_480"
;n = 300
;k = 50
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.5
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]



; Sparse CNN layers with a dense hidden layer identical to Dense CNN2
;[sparseCNNFC2Seeds]
;[sparseCNN2D3Seeds]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 16
;validation = 1.0
;use_batch_norm = False
;saveNet = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Same as Sparse CNN-2 except the hidden layer has weight sparsity = 1
;[sparseCNNFC2KSeeds]
;[sparseCNN2W1Seeds]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 300
;k = 50
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 16
;validation = 1.0
;use_batch_norm = False
;saveNet = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Sparse CNN-2 with a hidden layer like Dense CNN-2 but with weight sparsity 0.3
;[sparseCNN2DSWSeeds]
;[sparseCNNFC2WSSeeds]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 16
;validation = 1.0
;use_batch_norm = False
;saveNet = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]


; Two layer sparse CNN nets
;[sparseCNN2]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 300
;k = 50
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = False
;use_batch_norm = False

; Two layer sparse CNN nets, compute error bars over multiple seeds
;[bestSparseCNNTwoLayerSeeds]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 300
;k = 50
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = False
;use_batch_norm = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Two layer nets optimization. We increase the linear layer and check
; on validation data
;[twoLayerSparseCNNValidation]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = [300, 500, 800, 1000]
;k = [50, 100, 150]
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;saveNet = False
;use_batch_norm = False
;validation = float(50000.0/60000.0)


; Two layer nets optimization. We test sparsity of CNN layer 2
;[twoLayerSparseCNNSparserL2]
;c1_out_channels = "30_30"
;c1_k = ["400_300", "400_250"]
;n = 300
;k = 50
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;saveNet = False
;use_batch_norm = False
;validation = 1.0

; Two layer nets optimization. We test sparsity of CNN layer 2
;[twoLayerSparseCNNValidation2]
;c1_out_channels = "30_30"
;c1_k = ["400_300", "400_250", "400_200", "400_100", "400_150"]
;n = 300
;k = 50
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;saveNet = False
;use_batch_norm = False
;validation = float(50000.0/60000.0)

; Best version from validation stage was n=300 k=50.
; This runs the above experiment on the full training set.
;[bestTwoLayerSparseCNNs]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = [300, 500, 800, 1000]
;k = [50, 100, 150]
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = False

; Two layer nets dense
;[twoLayerDenseCNNs]
;c1_out_channels = "30_30"
;c1_k = "4320_480"
;experiment = list
;n = [300, 500, 800, 1000]
;k = [300, 500, 800, 1000]
;iterations = 15
;boost_strength = 0.0
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = False

; Best two layer nets dense, 10 random seeds
;[twoLayerDenseCNNSeeds]
;c1_out_channels = "30_30"
;c1_k = "4320_480"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 0.0
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]


; Best two layer nets dense, dropout
;[twoLayerDenseCNNDropout]
;c1_out_channels = "30_30"
;c1_k = "4320_480"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 0.0
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = False

; Best one layer net, multiple seeds
; Parameters chosen using validation set
;[DenseCNN1Seeds]
;c1_out_channels = "30"
;c1_k = "4320"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 0.0
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 16
;validation = 1.0
;saveNet = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]


; One CNN layer nets validation
; We use this to decide on Dense CNN-1 in the paper
;[oneLayerDenseCNNValidation]
;c1_out_channels = "30"
;c1_k = "4320"
;experiment = list
;n = [300, 500, 800, 1000]
;k = [300, 500, 800, 1000]
;iterations = 15
;boost_strength = 0.0
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = float(50000.0/60000.0)
;saveNet = False
;

; One CNN layer nets learning rate validation
; 0.01 was much better
;[oneLayerDenseCNNValidationLearningRate]
;c1_out_channels = "30"
;c1_k = "4320"
;experiment = list
;n = 300
;k = 300
;iterations = 15
;boost_strength = 0.0
;learning_rate = [0.01, 0.02]
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = float(50000.0/60000.0)
;saveNet = False

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; Various older experiments below
;

;[cnn6]
;c1_out_channels = [25,30,35]
;c1_k = [400,350,450]
;n = [100,150]
;k = 10
;iterations = 15
;boost_strength = 1.2
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = [0.8, 0.7]
;weight_sparsity = [0.4, 0.35, 0.3]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; How important is weight sparsity?
;[cnn7]
;c1_out_channels = 30
;c1_k = 400
;n = [100,150]
;k = 10
;iterations = 15
;boost_strength = 1.2
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.7
;weight_sparsity = [1.0, 0.8, 0.6, 0.4]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; Testing k
;[cnn8]
;c1_out_channels = 30
;c1_k = 450
;n = 150
;k = [150,50]
;iterations = 15
;boost_strength = 1.2
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.7
;weight_sparsity = [0.3]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; Testing n/k with momentum=0.25
;[cnn9]
;c1_out_channels = 30
;c1_k = [450, 400]
;n = [150, 300, 500]
;k = [50]
;iterations = 12
;boost_strength = 1.2
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.7
;weight_sparsity = [0.3, 0.2, 0.1]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; Testing learning rate and learning schedule
;[cnn10]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 50
;iterations = 15
;boost_strength = 1.2
;boost_strength_factor = 0.85
;momentum = 0.0
;learning_rate = [0.1, 0.075, 0.05, 0.025, 0.01]
;learning_rate_factor = [0.5, 0.6, 0.7, 0.8]
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = [4000, 2000, 1000]
;create_plots = False
;batch_size = 4


; Testing n/k, with momentum = 0
;[cnn11]
;c1_out_channels = 30
;c1_k = [450, 400]
;n = [150, 300, 500]
;k = [50]
;iterations = 12
;boost_strength = 1.2
;boost_strength_factor = 0.85
;learning_rate = 0.01
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = [0.3, 0.2, 0.1]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; Best of cnn9 with higher learning rate. Good accuracy. Iteration 5 has high
; accuracy (98.6) and decent noise (99K), but latter drops a bit afterwards.
;[cnn12]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 50
;iterations = 12
;boost_strength = 1.2
;boost_strength_factor = 0.85
;learning_rate = 0.02
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; Like 12 but slightly higher boost and learning rate, and more iterations
; Got best accuracy (98.56%) and noise combination (100K).
; The highest accuracy one (98.8%) did not have great noise robustness (95K)
;[cnn13]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 50
;iterations = 20
;boost_strength = [1.2, 1.3, 1.4]
;boost_strength_factor = 0.85
;learning_rate = [0.03, 0.02]
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; Like 13 but higher boost and learning rates
; These got great accuracy (98.8%) but low noise robustness
;[cnn14]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 50
;iterations = 12
;boost_strength = [1.3, 1.4, 1.5]
;boost_strength_factor = 0.85
;learning_rate = [0.05, 0.04]
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; Like cnn14 but increasing N to get higher sparsity
;[cnn15]
;c1_out_channels = 30
;c1_k = 400
;n = [200, 250, 300]
;k = [25, 50]
;iterations = 10
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.05
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4


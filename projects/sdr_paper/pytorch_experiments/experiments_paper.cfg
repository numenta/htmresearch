# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

#
# Parameters used in the paper
#

[DEFAULT]
# Keep final results in a separate location
path = results

experiment = grid
repetitions = 1
; Uncomment to average over multiple seeds
;repetitions = 10

seed = 42
datadir = data
lr_scheduler = StepLR
lr_scheduler_params = "{'step_size': 1, 'gamma':%(learning_rate_factor)s}"
validation = 1.0
no_cuda = False
batches_in_epoch = 100000
log_interval = 1000
test_batch_size = 1000
create_plots = False
count_nonzeros = True
test_noise_every_epoch = True

use_batch_norm = False
boost_strength_factor = 1.0
optimizer = SGD
c1_input_shape = "1_28_28"

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the main networks described in the MNIST section
; of the paper.


;[denseCNN1]
;c1_out_channels = "30"
;c1_k = "4320"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 0.0
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;create_plots = False
;batch_size = 16
;validation = 1.0
;saveNet = True

;[denseCNN2]
;c1_out_channels = "30_30"
;c1_k = "4320_480"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 0.0
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;create_plots = False
;batch_size = 16
;validation = 1.0
;saveNet = True

;[sparseCNN1]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 50
;iterations = 20
;boost_strength = 1.4
;boost_strength_factor = 0.85
;learning_rate = 0.02
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = True


;[sparseCNN2]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 300
;k = 50
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;create_plots = False
;batch_size = 4
;validation = 1.0
;use_batch_norm = False
;saveNet = True


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the mixed networks described in the MNIST section
; of the paper.

; Dense CNN layers with a sparse hidden layer identical to Sparse CNN2
;[denseCNN2SP3]
;c1_out_channels = "30_30"
;c1_k = "4320_480"
;n = 300
;k = 50
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.5
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = False

; Sparse CNN layers with a dense hidden layer like Dense CNN2
;[sparseCNN2D3]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;create_plots = False
;batch_size = 16
;validation = 1.0
;use_batch_norm = False
;saveNet = False

; Same as Sparse CNN-2 except the hidden layer has weight sparsity = 1
;[sparseCNN2W1]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 300
;k = 50
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;create_plots = False
;batch_size = 16
;validation = 1.0
;use_batch_norm = False
;saveNet = False

; Sparse CNN-2 with a hidden layer like Dense CNN-2 but with weight sparsity 0.3
;[sparseCNN2DSW]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;create_plots = False
;batch_size = 16
;validation = 1.0
;use_batch_norm = False
;saveNet = False

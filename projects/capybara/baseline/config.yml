inputs:
  input_dir: '../htm/traces'
  # Note: 'baseName' is used to form the CSV input file names. 
  # E.g: '<baseName>_train.csv' and '<baseName>_test.csv'
  base_name: 'inertial_signals' 
  metric_name: 'body_acc_x'

outputs:
  results_output_dir: 'results'
  model_output_dir: 'model'
  history_file: 'train_history.csv'
  prediction_file: 'predictions.csv'
  model_name: 'baseline.h5'

params:
  chunk_size: 2048
  batch_size: 32
  num_epochs: 10
  ma_window: 10
  # 'tmPredictedActiveCells' or 'spActiveColumns'
  input_name: 'tmPredictedActiveCells' 
  input_dim: 65536 # 2048 * 32 = 65536
  output_dim: 6
  labels: ['WALKING',
          'WALKING_UPSTAIRS',
          'WALKING_DOWNSTAIRS',
          'SITTING',
          'STANDING',
          'LAYING']
  # If lazy=True, don't load all the data in memory and train the model chunk
  # by chunk. Repeat for each epoch. (Memory efficient but slower because 
  # at each epoch, the lazy panda data frame iterator needs to re-created).
  # If lazy=False, load all the data in memory and train the model for 
  # multiple epochs (Memory intensive, but faster - since the panda data 
  # frame is only loaded at the beginning).
  lazy: True
  # If train=True, train a model.
  # If train=False, load a model from disk.
  train: True 
  


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 3-NYCtaxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from nupic.encoders import ScalarEncoder\n",
    "from nupic.bindings.algorithms import TemporalMemory as TM\n",
    "from nupic.bindings.algorithms import SpatialPooler as SP\n",
    "from htmresearch.support.neural_correlations_utils import *\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputSize = 109\n",
    "maxItems = 17520\n",
    "tmEpochs = 1\n",
    "totalTS = maxItems * tmEpochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv('nyc_taxi.csv', skiprows=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tm = TM(columnDimensions = (2048,),\n",
    "        cellsPerColumn=8, # We changed here the number of cells per col, initially they were 32\n",
    "        initialPermanence=0.21,\n",
    "        connectedPermanence=0.3,\n",
    "        minThreshold=15,\n",
    "        maxNewSynapseCount=40,\n",
    "        permanenceIncrement=0.1,\n",
    "        permanenceDecrement=0.1,\n",
    "        activationThreshold=15,\n",
    "        predictedSegmentDecrement=0.01\n",
    "       )\n",
    "\n",
    "sparsity = 0.02\n",
    "sparseCols = int(tm.numberOfColumns() * sparsity)\n",
    "\n",
    "sp = SP(inputDimensions=(inputSize,),\n",
    "        columnDimensions=(2048,),\n",
    "        potentialRadius = int(0.5*inputSize),\n",
    "        numActiveColumnsPerInhArea = sparseCols,\n",
    "        potentialPct = 0.9,\n",
    "        globalInhibition = True,\n",
    "        synPermActiveInc = 0.0001,\n",
    "        synPermInactiveDec = 0.0005,\n",
    "        synPermConnected = 0.5,\n",
    "        maxBoost = 1.0,\n",
    "        spVerbosity = 1\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawValues = []\n",
    "remainingRows = maxItems\n",
    "numTrainingItems = 15000\n",
    "trainSet = []\n",
    "nonTrainSet = []\n",
    "\n",
    "se = ScalarEncoder(n=109, w=29, minval=0, maxval=40000, clipInput=True)\n",
    "s = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if s > 0 and s % 500 == 0:\n",
    "        print str(s) + \" items processed\"\n",
    "        \n",
    "    rawValues.append(row['passenger_count'])\n",
    "    \n",
    "    if s < numTrainingItems:\n",
    "        trainSet.append(se.encode(row['passenger_count']))\n",
    "    else:\n",
    "        nonTrainSet.append(se.encode(row['passenger_count']))\n",
    "        \n",
    "    remainingRows -= 1\n",
    "    s += 1\n",
    "    if remainingRows == 0: \n",
    "        break\n",
    "print \"*** All items encoded! ***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Spatial Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allSequences = []\n",
    "outputColumns = np.zeros(sp.getNumColumns(), dtype=\"uint32\")\n",
    "columnUsage = np.zeros(sp.getNumColumns(), dtype=\"uint32\")\n",
    "\n",
    "# Set epochs for spatial-pooling:\n",
    "spEpochs = 4\n",
    "\n",
    "for epoch in range(spEpochs):\n",
    "    print \"Training epoch: \" + str(epoch)\n",
    "    \n",
    "    #randomize records in training set\n",
    "    randomIndex = np.random.permutation(np.arange(numTrainingItems))\n",
    "    \n",
    "    for i in range(numTrainingItems):\n",
    "        sp.compute(trainSet[randomIndex[i]], True, outputColumns)\n",
    "        # Populate array for Yuwei plot:\n",
    "        for col in outputColumns.nonzero():\n",
    "            columnUsage[col] += 1                        \n",
    "        if epoch == (spEpochs - 1):\n",
    "            allSequences.append(outputColumns.nonzero()) \n",
    "\n",
    "for i in range(maxItems - numTrainingItems):\n",
    "    if i > 0 and i % 500 == 0:\n",
    "        print str(i) + \" items processed\"    \n",
    "    sp.compute(nonTrainSet[i], False, outputColumns)\n",
    "    allSequences.append(outputColumns.nonzero())\n",
    "    # Populate array for Yuwei plot:\n",
    "    for col in outputColumns.nonzero():\n",
    "        columnUsage[col] += 1                \n",
    "\n",
    "print \"*** All items processed! ***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 50\n",
    "plt.hist(columnUsage, bins)\n",
    "plt.xlabel(\"Number of times active\")\n",
    "plt.ylabel(\"Number of columns\")\n",
    "plt.savefig(\"columnUsage_SP\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Temporal Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spikeTrains = np.zeros((tm.numberOfCells(), totalTS), dtype = \"uint32\")\n",
    "columnUsage = np.zeros(tm.numberOfColumns(), dtype=\"uint32\")\n",
    "spikeCount = np.zeros(totalTS, dtype=\"uint32\")\n",
    "ts = 0\n",
    "\n",
    "entropyX = []\n",
    "entropyY = []\n",
    "\n",
    "negPCCX_cells = []\n",
    "negPCCY_cells = []\n",
    "\n",
    "numSpikesX = []\n",
    "numSpikesY = []\n",
    "\n",
    "numSpikes = 0\n",
    "\n",
    "negPCCX_cols = []\n",
    "negPCCY_cols = []\n",
    "\n",
    "# Randomly generate the indices of the columns to keep track during simulation time\n",
    "colIndicesLarge = np.random.permutation(tm.numberOfColumns())[0:125] # keep track of 125 columns = 1000 cells\n",
    "\n",
    "for e in range(tmEpochs):\n",
    "    print \"\"\n",
    "    print \"Epoch: \" + str(e)\n",
    "    for s in range(maxItems):\n",
    "        if s % 1000 == 0:\n",
    "            print str(s) + \" items processed\"\n",
    "        \n",
    "        tm.compute(allSequences[s][0].tolist(), learn=True)\n",
    "        for cell in tm.getActiveCells():\n",
    "            spikeTrains[cell, ts] = 1 \n",
    "            numSpikes += 1\n",
    "            spikeCount[ts] += 1\n",
    "            \n",
    "        # Obtain active columns:\n",
    "        activeColumnsIndices = [tm.columnForCell(i) for i in tm.getActiveCells()]\n",
    "        currentColumns = [1 if i in activeColumnsIndices else 0 for i in range(tm.numberOfColumns())]\n",
    "        for col in np.nonzero(currentColumns)[0]:\n",
    "            columnUsage[col] += 1                \n",
    "    \n",
    "        if ts > 0 and ts % int(totalTS * 0.1) == 0:\n",
    "            numSpikesX.append(ts)\n",
    "            numSpikesY.append(numSpikes)\n",
    "            \n",
    "            numSpikes = 0            \n",
    "            subSpikeTrains = subSample(spikeTrains, 1000, tm.numberOfCells(), ts)\n",
    "            (corrMatrix, numNegPCC) = computePWCorrelations(subSpikeTrains, removeAutoCorr=True)\n",
    "            negPCCX_cells.append(ts)\n",
    "            negPCCY_cells.append(numNegPCC)                \n",
    "            bins = 300\n",
    "            plt.hist(corrMatrix.ravel(), bins, alpha=0.5)                \n",
    "            # Set range for plot appropriately!\n",
    "            plt.xlim(-0.1,0.2)\n",
    "            plt.xlabel(\"PCC\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.savefig(\"cellsHist\" + str(ts))\n",
    "            plt.close()\n",
    "            entropyX.append(ts)\n",
    "            entropyY.append(computeEntropy(subSpikeTrains))\n",
    "\n",
    "            #print \"++ Analyzing correlations (whole columns) ++\"\n",
    "            ### First the LARGE subsample of columns:\n",
    "            subSpikeTrains = subSampleWholeColumn(spikeTrains, colIndicesLarge, tm.getCellsPerColumn(), ts)\n",
    "            (corrMatrix, numNegPCC) = computePWCorrelations(subSpikeTrains, removeAutoCorr=True)\n",
    "            negPCCX_cols.append(s)\n",
    "            negPCCY_cols.append(numNegPCC)                \n",
    "            #print \"++ Generating histogram ++\"\n",
    "            bins = 300\n",
    "            plt.hist(corrMatrix.ravel(), bins, alpha=0.5)\n",
    "            plt.xlim(-0.05,0.1)\n",
    "            plt.xlabel(\"PCC\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.savefig(\"colsHist_\" + str(ts))\n",
    "            plt.close()                 \n",
    "\n",
    "        ts += 1                \n",
    "        \n",
    "print \"*** DONE ***\"\n",
    "# end for-epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparsityTraceX = []\n",
    "sparsityTraceY = []\n",
    "for i in range(totalTS - 1000):\n",
    "    sparsityTraceX.append(i)\n",
    "    sparsityTraceY.append(np.mean(spikeCount[i:1000 + i]) / tm.numberOfCells())\n",
    "\n",
    "plt.plot(sparsityTraceX, sparsityTraceY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Sparsity\")\n",
    "plt.savefig(\"sparsityTrace\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot trace of negative PCCs\n",
    "plt.plot(negPCCX_cells, negPCCY_cells)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Negative PCC Count\")\n",
    "plt.savefig(\"negPCCTrace_cells\")\n",
    "plt.close()\n",
    "\n",
    "plt.plot(negPCCX_cols, negPCCY_cols)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Negative PCC Count\")\n",
    "plt.savefig(\"negPCCTrace_cols\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print computeEntropy()\n",
    "plt.plot(entropyX, entropyY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.savefig(\"entropyTM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 50\n",
    "plt.hist(columnUsage, bins)\n",
    "plt.xlabel(\"Number of times active\")\n",
    "plt.ylabel(\"Number of columns\")\n",
    "plt.savefig(\"columnUsage_TM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(numSpikesX, numSpikesY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Num Spikes\")\n",
    "plt.savefig(\"numSpikesTrace\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Analysis of Spike Trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleAccuracyTest(\"periodic\", tm, allSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subSpikeTrains = subSample(spikeTrains, 1000, tm.numberOfCells(), totalTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isi = computeISI(subSpikeTrains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bins = np.linspace(np.min(isi), np.max(isi), 50)\n",
    "bins = 100\n",
    "plt.hist(isi, bins)\n",
    "# plt.xlim(0,4000)\n",
    "# plt.xlim(89500,92000)\n",
    "plt.xlabel(\"ISI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"isiTM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subSpikeTrains = subSample(spikeTrains, 100, tm.numberOfCells(), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rasterPlot(subSpikeTrains, \"TM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part V. Save TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saveTM(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to load the TM back from the file do:\n",
    "with open('tm.nta', 'rb') as f:\n",
    "    proto2 = TemporalMemoryProto_capnp.TemporalMemoryProto.read(f, traversal_limit_in_words=2**61)\n",
    "tm = TM.read(proto2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VI. Analysis of Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overlapMatrix = inputAnalysis(allSequences, \"periodic\", tm.numberOfColumns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show heatmap of overlap matrix\n",
    "plt.imshow(overlapMatrix, cmap='spectral', interpolation='nearest')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Overlap Score')\n",
    "plt.savefig(\"overlapScore_heatmap\")\n",
    "plt.close()\n",
    "# plt.show()\n",
    "\n",
    "# generate histogram\n",
    "bins = 60\n",
    "(n, bins, patches) = plt.hist(overlapMatrix.ravel(), bins, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Overlap Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"overlapScore_hist\")\n",
    "\n",
    "plt.xlim(0.2,1)\n",
    "plt.ylim(0,1000000)\n",
    "plt.xlabel(\"Overlap Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"overlapScore_hist_ZOOM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
